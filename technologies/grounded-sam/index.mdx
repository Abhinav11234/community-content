---
title: "Grounded-Segment-Anything"
description: "Grounded-Segment-Anything is a framework that combines Grounding DINO and Segment Anything to detect and segment objects in images using text prompts. The project also incorporates other models like Stable-Diffusion, Tag2Text, and BLIP for various tasks like image generation and automatic labeling."
---

# Grounded-Segment-Anything

Grounded-Segment-Anything is a framework that combines [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [Segment Anything](https://github.com/facebookresearch/segment-anything) to detect and segment objects in images using text prompts. The project also incorporates other models like [Stable-Diffusion](https://github.com/CompVis/stable-diffusion), [Tag2Text](https://github.com/xinyu1205/Tag2Text), and [BLIP](https://github.com/salesforce/lavis) for various tasks like image generation and automatic labeling.

| General  |  |
| --- | --- |
| Release date |  March 31, 2023 |
| Repository | https://github.com/IDEA-Research/Grounded-Segment-Anything |
| Type | Image Segmentation and Detection |

**ðŸ”¥ Highlighted Projects** 

- Checkout the [Automated Dataset Annotation and Evaluation with GroundingDINO and SAM](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb) which is an amazing tutorial on automatic labeling! Thanks a lot for [Piotr Skalski](https://github.com/SkalskiP) and [Robotflow](https://github.com/roboflow/notebooks)!
- Checkout the [Segment Everything Everywhere All at Once](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) demo! It supports segmenting with various types of prompts (text, point, scribble, referring image, etc.) and any combination of prompts.
- Checkout the [OpenSeeD](https://github.com/IDEA-Research/OpenSeeD) for the interactive segmentation with box input to generate mask.
- Visual instruction tuning with GPT-4! Please check out the multimodal model **LLaVA**: [[Project Page](https://llava-vl.github.io/)] [[Paper](https://arxiv.org/abs/2304.08485)] [[Demo](https://llava.hliu.cc/)]  [[Data](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)] [[Model](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0)]

---
